
@article{benjamini_controlling_1995,
	title = {Controlling the {False} {Discovery} {Rate}: {A} {Practical} and {Powerful} {Approach} to {Multiple} {Testing}},
	volume = {57},
	issn = {0035-9246},
	shorttitle = {Controlling the {False} {Discovery} {Rate}},
	url = {http://www.jstor.org/stable/2346101},
	abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
	number = {1},
	urldate = {2018-07-24},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Benjamini, Yoav and Hochberg, Yosef},
	year = {1995},
	pages = {289--300}
}

@book{grolemund_r_nodate,
	title = {R for {Data} {Science}},
	isbn = {978-1-4919-1039-9},
	url = {http://shop.oreilly.com/product/0636920034407.do},
	abstract = {Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for...},
	language = {en},
	urldate = {2018-08-22},
	author = {Grolemund, Garrett, Hadley Wickham},
	file = {Snapshot:/Users/Auri/Zotero/storage/TSEITPI2/0636920034407.html:text/html}
}

@book{wood_generalized_2017,
	title = {Generalized {Additive} {Models}: {An} {Introduction} with {R}, {Second} {Edition}},
	isbn = {978-1-4987-2837-9},
	shorttitle = {Generalized {Additive} {Models}},
	abstract = {The first edition of this book has established itself as one of the leading references on generalized additive models (GAMs), and the only book on the topic to be introductory in nature with a wealth of practical examples and software implementation. It is self-contained, providing the necessary background in linear models, linear mixed models, and generalized linear models (GLMs), before presenting a balanced treatment of the theory and applications of GAMs and related models.   The author bases his approach on a framework of penalized regression splines, and while firmly focused on the practical aspects of GAMs, discussions include fairly full explanations of the theory underlying the methods. Use of R software helps explain the theory and illustrates the practical application of the methodology. Each chapter contains an extensive set of exercises, with solutions in an appendix or in the book’s R data package gamair, to enable use as a course text or for self-study. Simon N. Wood is a professor of Statistical Science at the University of Bristol, UK, and author of the R package mgcv.},
	language = {en},
	publisher = {CRC Press},
	author = {Wood, Simon N.},
	month = may,
	year = {2017},
	note = {Google-Books-ID: JTkkDwAAQBAJ},
	keywords = {Mathematics / Probability \& Statistics / General}
}

@article{bland_statistical_1986,
	title = {Statistical methods for assessing agreement between two methods of clinical measurement},
	volume = {1},
	issn = {0140-6736},
	abstract = {In clinical measurement comparison of a new measurement technique with an established one is often needed to see whether they agree sufficiently for the new to replace the old. Such investigations are often analysed inappropriately, notably by using correlation coefficients. The use of correlation is misleading. An alternative approach, based on graphical techniques and simple calculations, is described, together with the relation between this analysis and the assessment of repeatability.},
	language = {eng},
	number = {8476},
	journal = {Lancet (London, England)},
	author = {Bland, J. M. and Altman, D. G.},
	month = feb,
	year = {1986},
	pmid = {2868172},
	keywords = {Humans, Statistics as Topic, Diagnosis, Peak Expiratory Flow Rate},
	pages = {307--310}
}

@book{wickham_ggplot2_2016,
	edition = {2},
	series = {Use {R}!},
	title = {ggplot2: {Elegant} {Graphics} for {Data} {Analysis}},
	isbn = {978-3-319-24275-0},
	shorttitle = {ggplot2},
	url = {//www.springer.com/it/book/9783319242750},
	abstract = {This new edition to the classic book by ggplot2 creator Hadley Wickham highlights compatibility with knitr and RStudio. ggplot2 is a data visualization package for R that helps users create data graphics, including those that are multi-layered, with ease. With ggplot2, it's easy to: produce handsome, publication-quality plots with automatic legends created from the plot specificationsuperimpose multiple layers (points, lines, maps, tiles, box plots) from different data sources with automatically adjusted common scalesadd customizable smoothers that use powerful modeling capabilities of R, such as loess, linear models, generalized additive models, and robust regressionsave any ggplot2 plot (or part thereof) for later modification or reusecreate custom themes that capture in-house or journal style requirements and that can easily be applied to multiple plotsapproach a graph from a visual perspective, thinking about how each component of the data is represented on the final plot This book will be useful to everyone who has struggled with displaying data in an informative and attractive way. Some basic knowledge of R is necessary (e.g., importing data into R). ggplot2 is a mini-language specifically tailored for producing graphics, and you'll learn everything you need in the book. After reading this book you'll be able to produce graphics customized precisely for your problems, and you'll find it easy to get graphics out of your head and on to the screen or page.},
	language = {en},
	urldate = {2018-08-22},
	publisher = {Springer International Publishing},
	author = {Wickham, Hadley},
	year = {2016},
	file = {Snapshot:/Users/Auri/Zotero/storage/ZFCAS68C/9783319242750.html:text/html}
}

@article{mills_modelling_2007,
	title = {Modelling the relationship between body fat and the {BMI}},
	volume = {5},
	issn = {1479-456X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3183503/},
	abstract = {Objective
Given the increasing concerns about the levels of obesity being reached throughout the world, this paper analyses the relationship between the most common index of obesity, the BMI, and levels of body fat.

Research methods and procedures
The statistical relationship, in terms of functional form, between body fat and BMI is analysed using a large data set which can be categorized by race, sex and age.

Results
Irrespective of race, body fat and BMI are linearly related for males, with age entering logarithmically and with a positive effect on body fat. Caucasian males have higher body fat irrespective of age, but African American males’ body fat increases with age faster than that of Asians and Hispanics. Age is not a significant predictor of body fat for females, where the relationship between body fat and BMI is nonlinear except for Asians. Caucasian females have higher predicted body fat than other races, except at low BMIs, where Asian females are predicted to have the highest body fat.

Discussion
Using BMIs to make predictions about body fat should be done with caution, as such predictions will depend upon race, sex and age and can be relatively imprecise. The results are of practical importance for informing the current debate on whether standard BMI cut-off values for overweight and obesity should apply to all sex and racial groups given that these BMI values are shown to correspond to different levels of adiposity in different groups.},
	number = {2},
	urldate = {2018-11-10},
	journal = {International journal of body composition research},
	author = {Mills, T.C. and Gallagher, D. and Wang, J. and Heshka, S.},
	year = {2007},
	pmid = {22049264},
	pmcid = {PMC3183503},
	pages = {73--79},
	file = {PubMed Central Full Text PDF:/Users/Auri/Zotero/storage/4YVNXGDB/Mills et al. - 2007 - Modelling the relationship between body fat and th.pdf:application/pdf}
}

@article{peduzzi_simulation_1996,
	title = {A simulation study of the number of events per variable in logistic regression analysis},
	volume = {49},
	issn = {0895-4356},
	abstract = {We performed a Monte Carlo study to evaluate the effect of the number of events per variable (EPV) analyzed in logistic regression analysis. The simulations were based on data from a cardiac trial of 673 patients in which 252 deaths occurred and seven variables were cogent predictors of mortality; the number of events per predictive variable was (252/7 =) 36 for the full sample. For the simulations, at values of EPV = 2, 5, 10, 15, 20, and 25, we randomly generated 500 samples of the 673 patients, chosen with replacement, according to a logistic model derived from the full sample. Simulation results for the regression coefficients for each variable in each group of 500 samples were compared for bias, precision, and significance testing against the results of the model fitted to the original sample. For EPV values of 10 or greater, no major problems occurred. For EPV values less than 10, however, the regression coefficients were biased in both positive and negative directions; the large sample variance estimates from the logistic model both overestimated and underestimated the sample variance of the regression coefficients; the 90\% confidence limits about the estimated values did not have proper coverage; the Wald statistic was conservative under the null hypothesis; and paradoxical associations (significance in the wrong direction) were increased. Although other factors (such as the total number of events, or sample size) may influence the validity of the logistic model, our findings indicate that low EPV can lead to major problems.},
	language = {eng},
	number = {12},
	journal = {Journal of Clinical Epidemiology},
	author = {Peduzzi, P. and Concato, J. and Kemper, E. and Holford, T. R. and Feinstein, A. R.},
	month = dec,
	year = {1996},
	pmid = {8970487},
	keywords = {Humans, Reproducibility of Results, Logistic Models, Regression Analysis, Coronary Disease, Coronary Artery Bypass, Computer Simulation, Bias, Monte Carlo Method},
	pages = {1373--1379}
}

@article{fisher_use_1936,
	title = {The {Use} of {Multiple} {Measurements} in {Taxonomic} {Problems}},
	volume = {7},
	copyright = {1936 Blackwell Publishing Ltd/University College London},
	issn = {2050-1439},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
	doi = {10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	language = {en},
	number = {2},
	urldate = {2019-10-09},
	journal = {Annals of Eugenics},
	author = {Fisher, R. A.},
	year = {1936},
	pages = {179--188},
	file = {Full Text PDF:/Users/Auri/Zotero/storage/MR2JAZFA/Fisher - 1936 - The Use of Multiple Measurements in Taxonomic Prob.pdf:application/pdf;Snapshot:/Users/Auri/Zotero/storage/TX8DL8SP/j.1469-1809.1936.tb02137.html:text/html}
}

@misc{noauthor_iris_2019,
	title = {\textit{{Iris}} flower data set},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Iris_flower_data_set&oldid=906180779},
	abstract = {The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gaspé Peninsula "all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus".The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.},
	language = {en},
	urldate = {2019-10-09},
	journal = {Wikipedia},
	month = jul,
	year = {2019},
	note = {Page Version ID: 906180779},
	file = {Snapshot:/Users/Auri/Zotero/storage/B4HG85KW/index.html:text/html}
}

@misc{noauthor_uci_nodate,
	title = {{UCI} {Machine} {Learning} {Repository}: {Iris} {Data} {Set}},
	url = {https://archive.ics.uci.edu/ml/datasets/iris},
	urldate = {2019-10-09},
	file = {UCI Machine Learning Repository\: Iris Data Set:/Users/Auri/Zotero/storage/S75A7ESR/iris.html:text/html}
}

@article{gorban_topological_2007,
	title = {Topological grammars for data approximation},
	volume = {20},
	issn = {0893-9659},
	url = {http://www.sciencedirect.com/science/article/pii/S0893965906001856},
	doi = {10.1016/j.aml.2006.04.022},
	abstract = {A method of topological grammars is proposed for multidimensional data approximation. For data with complex topology we define a principal cubic complex of low dimension and given complexity that gives the best approximation for the dataset. This complex is a generalization of linear and non-linear principal manifolds and includes them as particular cases. The problem of optimal principal complex construction is transformed into a series of minimization problems for quadratic functionals. These quadratic functionals have a physically transparent interpretation in terms of elastic energy. For the energy computation, the whole complex is represented as a system of nodes and springs. Topologically, the principal complex is a product of one-dimensional continuums (represented by graphs), and the grammars describe how these continuums transform during the process of optimal complex construction. This factorization of the whole process onto one-dimensional transformations using minimization of quadratic energy functionals allows us to construct efficient algorithms.},
	number = {4},
	urldate = {2019-10-09},
	journal = {Applied Mathematics Letters},
	author = {Gorban, A. N. and Sumner, N. R. and Zinovyev, A. Y.},
	month = apr,
	year = {2007},
	keywords = {Approximation, Cubic complex, Dataset, Elastic energy, Graph grammar, Principal component, Principal manifold},
	pages = {382--386},
	file = {ScienceDirect Snapshot:/Users/Auri/Zotero/storage/INT3G64A/S0893965906001856.html:text/html}
}
